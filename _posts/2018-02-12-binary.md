---
title: "A Quibble with 'Binary'"
date: 2018-02-12
author: Ryan Cordell
permalink: /personal/binary-quibble/
layout: post
---

*Preface: this is decidedly a* blog post*: a fragment of an idea that I might develop into something properly academic at some point. For now, these are more speculative ideas born from moments of annoyance over a series of recent presentations I've attended.*

One of the truisms about computation that I find most frustrating goes something like this:

> Of course, at its root, digital technology is *binary*. The switch is either flipped or it isn't. Data is assigned either this category or that.

Typically, this truism is invoked to make a point—or to more archly to *imply* a point—about the reductiveness of computational tools or models. How can a machine that only understands "yes" or "no" possibly grapple with the nuances of human experience? 

At a very fundamental level this truism is true, which helps explain why it is so pervasive and passively persuasive. At bottom, a computer processes information through logic gates, switches that are either "on" or "off," and this is why we speak of computers as binary machines. The deep, material substrate of the computer is binary. I won't belabor an explanation of this substrate here, in large part because I cannot do it justice. One of the best plain-language explanations I've come across is in Chapter 3, "The Language of Logic," of [Vikram Chandra's *Geek Sublime: The Beauty of Code, the Code of Beauty*](http://amzn.to/2EsfzwP). 

But such an account is also misleading, implying that the medium's individual binary components necessarily produce binary formations at the level of software and human interaction. They do not. The substrate of computation is not really *ever* a single logic gate, but instead a vast field of these binary switches—millions of them, in modern computers—each individually "on" or "off" but combining, in aggregate, to represent a vast many possibilities. Human beings interact with computers almost exclusively at aggregate, abstracted levels where notions of computation as exclusively, necessarily "binary" break down. Certainly programming languages are not in any meaningful sense "binary" systems, though they can be used to perform binary tasks. To rephrase: there are many ways computation can be reductive and exclusionary, but these effects don't spring inextricably from the computer's binarism. 

A useful metaphor here is print. We might argue that at its core print is also binary. Looking at sheets of paper through a microscope, we might observe that every fiber—or every segment of every fiber, depending on how we wanted to identify a single "unit" of the material page—is either *marked* or *not marked* by ink. But we do not interact with the printed page in such granular ways, any more than we interact with individual logic gates in our computers. Instead, we apprehend the inked and uninked portions of printed pages collectively, seeing the ways inked and uninked combine in patterns that represent letterforms or images. These patterns participate in even larger symbolic systems such as writing and artistic representation. In print, many "binary" page units combine to convey nuance and complexity (though writing, [as Socrates feared](http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0174%3Atext%3DPhaedrus%3Apage%3D275), is indeed a simplified representation of speech). 

When we talk about computation at the level of programs and data structures, binarism ceases to apply *as a necessary quality*. At some level programming languages do resolve to ones and zeros, but human beings use programming languages precisely because they abstract from complex, aggregate structures of ones and zeros to create a symbolic environment meaningful to humans. It is also true that many data structures categorize people in ways that essentialize, say, gender, allowing only a choice between "male" and "female." This is not because databases are "binary," however, but because the the people who designed those databases programmed them to understand only two, reductive categories of gender. Most database architectures would allow a designer to encode many more than two categories, as one option for encoding greater complexity and nuance. Alternatively, a data structure could be designed in such a way as to allow one individual to belong to multiple gender categories, or to allow users the ability to input their own categories. Data models and data formats can allow nuance and multiplicity if we prioritize them doing so. 

Consider a common task in computational text analysis: classification. To oversimplify a bit, classification tasks often look like this:

1. I have a corpus that contains different genres of text. I want to identify texts of a particular genre (say, poetry) from within that larger corpus. For an example of this kind of work, see [*Viral Texts* Project](http://viraltexts.org) Ph.D. student Jonathan Fitzgerald's research [classifying news, advertisements, poetry, and vignettes](http://jonathandfitzgerald.com/blog/2016/07/13/keystone-paper.html) from within our data on nineteenth-century reprinting.
2. I start by training my classifier on examples of the genre I want to identify. I tag lots of texts I know are poems as "poetry." These poems become training data which my classifier uses to build a model of "poetry." There are different methods for classification, some of which draw on vocabulary (what are the words that are most indicative of "poetry" based on the known poems provided?) and others of which draw on features such as topic models. But the first step to automatically classifying within a larger corpus is manually classifying a smaller corpus.
3. After training my classifier, I then run it on a set of texts of unknown genre. In my example thus far, the classifier will predict which of these are poetry and which are not. 

So here we have a task that looks binary: the classifier decides whether a given text *is poetry* or *is not poetry*. But not really. What the classifier actually does is assign a statistical likelihood to a given text being poetry, given the model by which it was trained to make such decisions.  One can decide that if that likelihood passes a certain threshold then the text should be sorted as "poetry," or one can decide to see the statistical scores assigned by the classifier. In [Fitz's post about his classification work](http://jonathandfitzgerald.com/blog/2016/07/13/keystone-paper.html) (and thanks to Fitz for helping me with the description that follows), he demonstrates classifiers that assign probabilities for individual texts across multiple genres. Fitz uses [logistic regression with a binary dependent variable](https://en.wikipedia.org/wiki/Logistic_regression), in which the probability for any text being, say, poetry can indeed be 0 (not poetry) or 1 (definitely poetry) or *any value in between 0 and 1*. In the experiment graphed below, Fitz tests each text against multiple genres, and each text is assigned a probability for each genre. 

![25 texts classified by multiple genres](http://jonathandfitzgerald.com/assets/KeystoneDH%20-%20Presentation.023.png)
*25 textual "clusters" (individual texts printed in multiple newspapers from the* Viral Texts *project) graphed by their probabilities of being advertisements, news, poetry, and/or prose.* 

In such a model of classification, a given text can be 95% likely to be news, but also 55% likely to be poetry. This model of classification raises all manner of interesting questions, both computational and literary-historical, and gives us a statistical vocabulary for talking about the way genres bleed into each other. What would it mean for a text strongly classified as "news" to also strongly resemble poetry? What lyricism lurks within the text, and how might that shift our perspective on it? Hoyt Long and Richard Jean so take up this interplay between computational analysis and expressive ambiguity in their recent ["Literary Pattern Recognition"](https://lucian.uchicago.edu/blogs/literarynetworks/files/2015/12/LONG_SO_CI.pdf), which uses the seeming "mistakes" of computational classification to discuss the latent "haikuness" of modernist poems that are not, on their face, haiku. As Fitz wrote in our exchange about his classification experiments, "the binary exists [in these methods] (0 or 1), but the classification method, logistic regression in this case, is sophisticated enough to measure between the binary."

My short reflection is growing long, and so I'll bring it to an end. To restate what I hope the post makes clear, I am not seeking to shut down conversation about the ways that computational models can be reductive or reinscribe political, social, or economic inequalities. The recent twitter uproar around [an IEEE historian's uniformed dismissal](https://www.insidehighered.com/news/2018/02/06/scholar-sets-twitter-furor-critiquing-book-he-hasnt-read) of Safiya Umoja Noble's *Algorithms of Oppression* is only one recent testament to the urgency of such conversations. My point is much smaller: a quibble with a rhetorical commonplace—almost a presentational tic—that misplaces the source of computational reductiveness into the machines themselves rather than in human choices. The problem is not that computers are "binary," but that we too often use them *as if* they were. The "computers are *binary*" truism is itself reductive and perhaps glosses over more substantive engagements we could be fostering between the humanities and computation.